{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Approaches to the Study of Food Sign Patterns\n",
    "Today, we're going to explore the ways in which restaurants of different cuisines describe food on their menus. Specifically, we're interested in the following questions about food cultures in Chicago:\n",
    "1. Are there patterns in the ways in which particular cuisine genres describe food on menus?\n",
    "    * Identification of indexical/iconic legisigns that position a particular cuisine within a cuisine, or social status (which can then be used by consumers to position themselves in the same light via social media posts, and so on)\n",
    "2. Are these cuisines (and/or menu discourse patterns identified in #1) geographically patterned?\n",
    "    * Identification of dicent indexical legisigns that point to a particular cuisine or broader menu discourse pattern on the basis of spatial location\n",
    "    \n",
    "First, let's load our packages. In order to run this notebook, you will need to install the `folium` package, which is available to install through the Anaconda Navigator or on the command line via the command `conda install -c conda-forge folium`\n",
    "\n",
    "# notes from class\n",
    "view HTML of website\n",
    "find menu items in the code in form of pseudo-json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "from sklearn.manifold import TSNE\n",
    "import folium\n",
    "\n",
    "# Some Functions from Last Time to get us started:\n",
    "def get_wordnet_pos(word):\n",
    "    import nltk\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def get_lemmas(text):\n",
    "    import nltk\n",
    "\n",
    "    # Combine list elements together into a single string for analysis\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    stop = nltk.corpus.stopwords.words('english') + list(string.punctuation) + [\"amp\", \"39\"]\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop]\n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def plot_top_tfidf(series, data_description):\n",
    "    import nltk\n",
    "\n",
    "    # Apply 'get lemmas' function to any Pandas Series that we pass in to get lemmas for each row in the Series\n",
    "    lemmas = series.apply(get_lemmas)\n",
    "\n",
    "    # Initialize Series of lemmas as Gensim Dictionary for further processing\n",
    "    dictionary = corpora.Dictionary([i for i in lemmas])\n",
    "\n",
    "    # Convert dictionary into bag of words format: list of (token_id, token_count) tuples\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "\n",
    "    # Calculate TFIDF based on bag of words counts for each token and return weights:\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    tfidf_weights = tfidf[bow_corpus[0]]\n",
    "\n",
    "    # Sort TFIDF weights highest to lowest:\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Plot the top 10 weighted words:\n",
    "    top_10 = {dictionary[k]:v for k,v in sorted_tfidf_weights[:10]} # dictionary comprehension\n",
    "    plt.plot(list(top_10.keys()), list(top_10.values()), label=data_description)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title('Top 10 Lemmas (TFIDF) for ' + data_description);\n",
    "\n",
    "    return\n",
    "\n",
    "html = requests.get(\"https://www.allmenus.com/il/chicago/22019-francescas-bryn-mawr/menu/\") #requests library\n",
    "# get response <200> means successful request\n",
    "html.text #see exact text of the website - all in text format so can't scrape because code itself is html\n",
    "#use beutiful soup library to parse file in terms of html and find particular tags that will allow us to take out menu and disregard irrelevant info\n",
    "soup = BeautifulSoup(html.text, \"html.parser\") #already loaded in from packages above - html.parser tells python that this is html and parse it out that way\n",
    "soup # easier to read because parsed correctly - now identical to when look at it on website itself - beautiful soup can find tags now\n",
    "# need to identify location where data lives in code to tell beautiful soup where to scrape\n",
    "# websites are putting data in json files because want google to be able to find them\n",
    "# want to tell beautiful soup to find area where script refers to \n",
    "restaurant_data = soup.find('script', type = 'application/ld+json') # returns everything between two tags in form of json\n",
    "#load as json b/c pythin doesn't know yet\n",
    "restaurant_data_json = json.loads(restaurant_data.text, strict = False) # strict = False b/c new lines \\n in text so telling it not to be too strict with that shit from htmls\n",
    "# actual menu data embedded in a bunch of different lists\n",
    "# e.g. \"has menu\" - if it has a menu, takes to further option inside list (has manu section) and then further to appetizers and then another nested item - need to get inside these nested lists\n",
    "#need long list comprehension to get into nested items\n",
    "nested_items = [section ['hasMenuItem'] for section in restaurant_data_json['hasMenu'][0]['hasMenuSection']]\n",
    "# about above: has menu has single list so index the zeroeth, within each section under has menu section -> end up with list of lists of dictionaries\n",
    "# get descriptions and name within single list\n",
    "names = [j['name'] for i in nested_items for j in i] #for each list in nested items and for each dictionary within those nested items, want to take name that is in that dictionary\n",
    "descriptions = [j['description'] for in in nested_items for j in i] #same for descriptions of items\n",
    "# but want to have data from many restaurants - need to either know the links for each menu or scrape links from allmenus and then scrape menus from each link\n",
    "# sort by italian in chicago (sorted by popularity) and grab top whatever number\n",
    "# since trying to scrape links, don't want grubhub links or other links\n",
    "# find in html text of all menus - the name of each restaurant and link associated with name\n",
    "html = requests.get('allmenus search link')\n",
    "soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "# restaurant list starts with tag \"ul\"\n",
    "soup.find('ul', class_= 'restaurant-list') # need underscore after class b/c just class is diff. python object\n",
    "# grubhub links have associated grubhub class specified but restaurant links don't have class associated with them so can use:\n",
    "restaurant_anchors = soup.find('ul', class_= 'restaurant-list').findAll('a', class_=None)\n",
    "# need to grab links without anchor data\n",
    "restaurant_links = ['first part of link' + i.get('href') for i in restaurant_anchors[:5]] # links in html are just the ends of links, so need to append the base of the link + all the href items in the text (refer to the links), took first five\n",
    "\n",
    "\n",
    "# from some other script that he wrote before class but is for some reason not here\n",
    "menu_df = pd.read_json('menu_df_top100.json')\n",
    "# assign map m calling folium, center map on chicago, set tiles to base map b/c it looks nice\n",
    "folium.Marker(menu_df['Coordinates'][1]).add_to(m) # created marker, added coordinates for the first entry and plotted them on m\n",
    "# create function to create map - take in dataframe of menus and output point map\n",
    "# to color individual points, loop through each point and assign a color to each (for loop in function)\n",
    "# asssign a different color to each cuisine using a dictionary\n",
    "# key also has code to add pop-up content for when you click on a point\n",
    "# for loop within function: create dictionary assigning colors to cuisines, then loop through each restaurant and add the color, then loop through and add the marker to the map with graphics stuff\n",
    "# legend is written in html because folium makes it hard to add legend\n",
    "\n",
    "# word to vec - words into vectors to see how words are used in sentences\n",
    "# model stuff \n",
    "# min_count: minimum occurances to consider\n",
    "# window: only consider x number of items around the word\n",
    "# workers: how many threads can be running at one time on a computer\n",
    "model.vocab # different words that made it into list and vector associated with each\n",
    "model['garlic'] # see actual structure of vector for specific word - 100 dimensional vector\n",
    "# reduce vector into 2D\n",
    "#list of labels and list of tokens\n",
    "# append 100-dimensional vector to list of tokens\n",
    "# append actual words to list of labels\n",
    "# define tsne model, n_components is two because reducing dimensionality to 2\n",
    "# specify x and y values b/c now in 2d\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
