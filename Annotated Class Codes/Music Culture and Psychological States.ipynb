{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Culture and Psychological States\n",
    "## Replicating and Expanding DeWall et al. 2011 \"Tuning in to Psychological Change\"<sup>1</sup>\n",
    "\n",
    "DeWall et al. 2011 tested three music lyric legisign relationships for evidence that replication of self-focus and antisocial legisigns has increased over time in song lyrics from the Billboard Hot 100 and that the replication of other-focus, social interactions, and positive emotion legisigns has decreased. Confirming their expectations, they argue that this pattern in musical legisign replication can be generalized to say that U.S. culture has become more self-focused and antisocial. If we accept Frith's assertion that songs \"provide people with the means to articulate [...] feelings\" (1996: 164)<sup>2</sup>, then this finding of increasingly antisocial and self-focused language within the most popular U.S. songs should be suggestive of a more general cultural psychological state. In Peircean terms, this means that, on one level, each word in the lyrics would be a replica of a dicent indexical legisign. For instance, replications of \"self-focus\" I/me/my/mine words stand for singer singing the word (and for listener who uses the word to articulate their own feelings), indexing their internal psychological state, and producing a dicent interpretant associating that psychological state with themselves. Today, we're going to attempt to replicate and expand DeWall et al.'s study (using Billboard Hot 100 song lyrics from 1950-2015) by answering the following questions:\n",
    "\n",
    "1. Can we identify increasing self-focus (first person singular pronouns) and decreasing other-focus (\"communion,\" first person plural pronouns)?\n",
    "2. Can we identify decreasing \"social connection\" legisign replication?\n",
    "3. Is there an increase in antisocial legisign replication and a decrease in pro-social legisign replication?\n",
    "4. Extending DeWall et al.'s study, are the topics and themes in songs changing as well? Do topics of music also reflect this anti-social, self-focus interpretation?\n",
    "\n",
    "Our data comes from a couple (see [here](https://github.com/kevinschaich/billboard) and [here](https://towardsdatascience.com/billboard-hot-100-analytics-using-data-to-understand-the-shift-in-popular-music-in-the-last-60-ac3919d39b49)) of public Github repositories that compiled extensive historical data about the Billboard Hot 100 from 1950-2015, along with data about the music itself from Spotify.\n",
    "    \n",
    "Note: The authors of the study tested individual words, but do not provide all of the words they used for Social Connection, Angry, Positive Emotion (unless the few words provided in the text on page 3 are the only words they checked). In the original study, DeWall et al. consider music genre as dummy variables in their regression models, as well as changes in ranking formula  to account for digital downloads and streamed media. We will not consider these factors for analysis as genre is a bit tricky to pin down because so many songs cross genre-boundaries and this information is often not available in early songs from the 50s. We will instead consider the overall effects across genres and ranking formulas.\n",
    "    \n",
    "---------------------------------\n",
    "  \n",
    "<sup>1</sup>DeWall, C. N., Pond, R. S., Jr., Campbell, W. K., & Twenge, J. M. 2011. \"Tuning in to Psychological Change: Linguistic Markers of Psychological Traits and Emotions Over Time in Popular U.S. Song Lyrics.\" *Psychology of Aesthetics*, Creativity, and the Arts.\n",
    "\n",
    "<sup>2</sup>Frith, Simon. 1996. “Songs as Texts.” In *Performing Rites: On the Value of Popular Music*. Cambridge,\n",
    "MA: Harvard University Press, pp. 158-182.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Some Functions from Last Time to get us started:\n",
    "def get_wordnet_pos(word):\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def get_lemmas(text):\n",
    "\n",
    "    stop = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop]\n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
    "    return lemmas\n",
    "\n",
    "# eliminate stop words item in get_lemmas so we can retain all the I's and Me's because we need them now\n",
    "def get_tokens(text):\n",
    "    # drop punctuation, but keep stopwords for initial word counting\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower())]\n",
    "    return tokens\n",
    "\n",
    "def fill_topic_weights(df_row, bow_corpus):\n",
    "    # Fill topic weights for topics in songs\n",
    "    try:\n",
    "        for i in ldamodel[bow_corpus[df_row.name]]:\n",
    "            df_row[str(i[0])] = i[1]\n",
    "    except:\n",
    "        return df_row\n",
    "    return df_row\n",
    "\n",
    "def top_songs_by_topic(ldamodel, corpus, ntop=1): # collect top end songs for a given topic\n",
    "    topn_songs_by_topic = {}\n",
    "    for i in range(len(ldamodel.print_topics())):\n",
    "        # For each topic, collect the most representative song(s) (i.e. highest probability containing words belonging to topic):\n",
    "        top = sorted(zip(range(len(corpus)), ldamodel[corpus]), reverse=True, key=lambda x: abs(dict(x[1]).get(i, 0.0)))\n",
    "        topn_songs_by_topic[i] = [j[0] for j in top[:ntop]]\n",
    "        # Print out the topn songs for each topic and return their indices as a dictionary for further analysis:\n",
    "        print(\"Topic \" + str(i))\n",
    "        print(music_df[['title','year','artist']].loc[topn_songs_by_topic[i]])\n",
    "        print(\"*******************************\")\n",
    "    return topn_songs_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = pd.read_csv('music_df.csv')\n",
    "# adjust variable (year bin) because labels are not great (50's instead of 1950s)\n",
    "music_df['year_bin'] = music_df['year_bin'].apply(lambda x: '20'+x if (x == '10s') or (x == '00s') else '19'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lyrics', 'num_syllables', 'pos', 'year', 'fog_index', 'flesch_index',\n",
      "       'num_words', 'num_lines', 'title', 'f_k_grade', 'artist',\n",
      "       'difficult_words', 'num_dupes', 'neg', 'neu', 'compound', 'id',\n",
      "       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
      "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
      "       'duration_ms', 'time_signature', 'uri', 'analysis_url',\n",
      "       'artist_with_features', 'year_bin', 'image', 'cluster', 'Gender'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>pos</th>\n",
       "      <th>year</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>flesch_index</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_lines</th>\n",
       "      <th>title</th>\n",
       "      <th>f_k_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>uri</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>artist_with_features</th>\n",
       "      <th>year_bin</th>\n",
       "      <th>image</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mona Lisa, Mona Lisa, men have named you\\nYou'...</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.199</td>\n",
       "      <td>1950</td>\n",
       "      <td>5.2</td>\n",
       "      <td>88.74</td>\n",
       "      <td>145</td>\n",
       "      <td>17</td>\n",
       "      <td>Mona Lisa</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>86.198</td>\n",
       "      <td>207573.0</td>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:3k5ycyXX5qsCjLd7R2vphp</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3k5y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950s</td>\n",
       "      <td>https://i.scdn.co/image/a4c0918f13b67aa8d9f4ea...</td>\n",
       "      <td>String Lover</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanna be Loved\\nBy Andrews Sisters\\n\\nOooo-o...</td>\n",
       "      <td>270.9</td>\n",
       "      <td>0.224</td>\n",
       "      <td>1950</td>\n",
       "      <td>4.4</td>\n",
       "      <td>82.31</td>\n",
       "      <td>189</td>\n",
       "      <td>31</td>\n",
       "      <td>I Wanna Be Loved</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>170.869</td>\n",
       "      <td>198027.0</td>\n",
       "      <td>5</td>\n",
       "      <td>spotify:track:4UY81WrDU3jTROGaKuz4uZ</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4UY8...</td>\n",
       "      <td>Gordon Jenkins</td>\n",
       "      <td>1950s</td>\n",
       "      <td>https://i.scdn.co/image/42e4dc3ab9b190056a1ca1...</td>\n",
       "      <td>String Lover</td>\n",
       "      <td>Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was dancing with my darling to the Tennessee...</td>\n",
       "      <td>174.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>1950</td>\n",
       "      <td>5.2</td>\n",
       "      <td>88.74</td>\n",
       "      <td>138</td>\n",
       "      <td>16</td>\n",
       "      <td>Tennessee Waltz</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>86.335</td>\n",
       "      <td>182733.0</td>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:6DKt9vMnMN0HmlnK3EAHRQ</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6DKt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950s</td>\n",
       "      <td>https://i.scdn.co/image/353b05113b1a140d64d83d...</td>\n",
       "      <td>String Lover</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Each time I hold someone new\\nMy arms grow col...</td>\n",
       "      <td>135.9</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1950</td>\n",
       "      <td>4.4</td>\n",
       "      <td>99.23</td>\n",
       "      <td>117</td>\n",
       "      <td>18</td>\n",
       "      <td>I'll Never Be Free</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>82.184</td>\n",
       "      <td>158000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:0KnD456yC5JuweN932Ems3</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0KnD...</td>\n",
       "      <td>Kay Starr</td>\n",
       "      <td>1950s</td>\n",
       "      <td>https://i.scdn.co/image/4bd427bb9181914d0fa448...</td>\n",
       "      <td>String Lover</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unfortunately, we are not licensed to display ...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>0.079</td>\n",
       "      <td>1950</td>\n",
       "      <td>6.0</td>\n",
       "      <td>69.79</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>All My Love</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>123.314</td>\n",
       "      <td>190933.0</td>\n",
       "      <td>4</td>\n",
       "      <td>spotify:track:05sXHTLqIpwywbpui1JT4o</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/05sX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950s</td>\n",
       "      <td>https://i.scdn.co/image/353b05113b1a140d64d83d...</td>\n",
       "      <td>String Lover</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  num_syllables    pos  \\\n",
       "0  Mona Lisa, Mona Lisa, men have named you\\nYou'...          189.0  0.199   \n",
       "1  I wanna be Loved\\nBy Andrews Sisters\\n\\nOooo-o...          270.9  0.224   \n",
       "2  I was dancing with my darling to the Tennessee...          174.6  0.351   \n",
       "3  Each time I hold someone new\\nMy arms grow col...          135.9  0.231   \n",
       "4  Unfortunately, we are not licensed to display ...           46.8  0.079   \n",
       "\n",
       "   year  fog_index  flesch_index  num_words  num_lines               title  \\\n",
       "0  1950        5.2         88.74        145         17           Mona Lisa   \n",
       "1  1950        4.4         82.31        189         31    I Wanna Be Loved   \n",
       "2  1950        5.2         88.74        138         16     Tennessee Waltz   \n",
       "3  1950        4.4         99.23        117         18  I'll Never Be Free   \n",
       "4  1950        6.0         69.79         32          3         All My Love   \n",
       "\n",
       "   f_k_grade  ...    tempo  duration_ms  time_signature  \\\n",
       "0        2.9  ...   86.198     207573.0               3   \n",
       "1        3.3  ...  170.869     198027.0               5   \n",
       "2        2.9  ...   86.335     182733.0               3   \n",
       "3        0.9  ...   82.184     158000.0               3   \n",
       "4        6.0  ...  123.314     190933.0               4   \n",
       "\n",
       "                                    uri  \\\n",
       "0  spotify:track:3k5ycyXX5qsCjLd7R2vphp   \n",
       "1  spotify:track:4UY81WrDU3jTROGaKuz4uZ   \n",
       "2  spotify:track:6DKt9vMnMN0HmlnK3EAHRQ   \n",
       "3  spotify:track:0KnD456yC5JuweN932Ems3   \n",
       "4  spotify:track:05sXHTLqIpwywbpui1JT4o   \n",
       "\n",
       "                                        analysis_url  artist_with_features  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/3k5y...                   NaN   \n",
       "1  https://api.spotify.com/v1/audio-analysis/4UY8...        Gordon Jenkins   \n",
       "2  https://api.spotify.com/v1/audio-analysis/6DKt...                   NaN   \n",
       "3  https://api.spotify.com/v1/audio-analysis/0KnD...             Kay Starr   \n",
       "4  https://api.spotify.com/v1/audio-analysis/05sX...                   NaN   \n",
       "\n",
       "  year_bin                                              image       cluster  \\\n",
       "0    1950s  https://i.scdn.co/image/a4c0918f13b67aa8d9f4ea...  String Lover   \n",
       "1    1950s  https://i.scdn.co/image/42e4dc3ab9b190056a1ca1...  String Lover   \n",
       "2    1950s  https://i.scdn.co/image/353b05113b1a140d64d83d...  String Lover   \n",
       "3    1950s  https://i.scdn.co/image/4bd427bb9181914d0fa448...  String Lover   \n",
       "4    1950s  https://i.scdn.co/image/353b05113b1a140d64d83d...  String Lover   \n",
       "\n",
       "   Gender  \n",
       "0    male  \n",
       "1   Group  \n",
       "2  female  \n",
       "3    male  \n",
       "4  female  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(music_df.columns)\n",
    "music_df.head()\n",
    "# today just working with lyrics, year and year bin variables - \n",
    "# there are a bunch of others that spotify and github authors have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write functions to answer questions\n",
    "# need to tokenize lyrics first\n",
    "lyrics = music_df['lyrics'].apply(get_tokens) #apply function to lyrics column\n",
    "# series of functions that we can apply to lyrics to determine percentages of each thing in questions\n",
    "def first_p_sg_perc(text): # self fcous (first person singular pronouns)\n",
    "    first_p_sg_count = text.count(\"i\") + text.count(\"me\") + text.count(\"my\") + text.count(\"mine\")\n",
    "    return first_p_sg_count/len(text)\n",
    "\n",
    "def first_p_pl_perc(text): # first person plural (other person focus)\n",
    "    first_p_pl_count = text.count(\"we\") + text.count(\"us\") + text.count(\"our\") + text.count(\"ours\")\n",
    "    return first_p_pl_count/len(text)\n",
    "\n",
    "# should be increase in first person singular and decrease in first person plural\n",
    "\n",
    "#scial connection\n",
    "def social_connection_perc(text):\n",
    "    social_count = text.count(\"mate\") + text.count(\"talk\") + text.count(\"child\") + text.count(\"together\") + text.count(\"friend\")\n",
    "    return social_count/len(text)\n",
    "# word2vec - find words that are related (clusters) and then identify how many there are'\n",
    "\n",
    "def antisocial_perc(text):\n",
    "    antisocial_count = text.count(\"kill\") + text.count(\"hate\") + text.count(\"annoyed\") + text.count(\"damn\") + text.count(\"fuck\")\n",
    "    return antisocial_count/len(text)\n",
    "\n",
    "def positive_perc(text):\n",
    "    positive_count = text.count(\"love\") + text.count(\"nice\") + text.count(\"sweet\")\n",
    "    return positive_count/len(text)\n",
    "\n",
    "agency = lyrics.apply(first_p_sg_perc) # self focus score\n",
    "communion = lyrics.apply(first_p_pl_perc) # other focus\n",
    "social_connection = lyrics.apply(social_connection_perc)\n",
    "antisocial = lyrics.apply(antisocial_perc)\n",
    "positive = lyrics.apply(positive_perc)\n",
    "\n",
    "#put them all into df by entering as dictionry values\n",
    "df = pd.DataFrame({'Agency': agency, 'Communion' : communion,\n",
    "                   'Social Connection': social_connection, 'Antisocial': antisocial,\n",
    "                   'Positive': positive, 'Year': music_df['year'], 'Year_Bin': music_df['year_bin']})\n",
    "\n",
    "# plot relationships to see visual patterns\n",
    "# plot by decade, not year because it's too noisy\n",
    "df[['Agency', 'Communion', 'Year_Bin']].groupby('Year_Bin').mean().plot(); # doule brackets mean that I'm taking columns and returning as a dataframe, groupby groups by particular item\n",
    "# also plot the others\n",
    "df[['Social Connection', 'Year_Bin']].groupby('Year_Bin').mean().plot();\n",
    "df[['Antisocial', 'Positive', 'Year_Bin']].groupby('Year_Bin').mean().plot();\n",
    "\n",
    "# look at regression equations\n",
    "# fit regression equations by year and year bin - repeat for all diff measures (only copied some)\n",
    "\n",
    "import statsmodels.formula.api as smf # formula api to write nice formulas\n",
    "\n",
    "mod = smf.ols(formula = 'Agency ~ Year', data = df) #ols for ordinary least squares\n",
    "res = mod.fit() # fit line\n",
    "print(res.summary()) # get summary\n",
    "\n",
    "mod = smf.ols(formula = 'Agency ~ Year_Bin', data = df) #ols for ordinary least squares\n",
    "res = mod.fit() # fit line\n",
    "print(res.summary()) # get summary\n",
    "\n",
    "\n",
    "# apart from using word2vec, can assess thematically if things are changing in songs as a whole - systemic shifts\n",
    "# can get after this using topic modeling\n",
    "# to do this, need to get data into form that can work with in gensim - lemmetize data using get_lemmas formula\n",
    "# not lemmetize in class because it takes a long time so read in previously lemmetized data\n",
    "lemmas = pd.read_pickle('lyric_lemmas.pkl')\n",
    "lemmas [0][:10] # see ten first lemmas of first song (nat king cole)\n",
    "# mona lisa is being separated as two things - can deal with this using word2vec or gensims built-in bigram finder\n",
    "# bigrams: two words that tend to cooccur and then combine into single unit to analyze as such\n",
    "bigram = models.Phrases(lemmas, min_count = 5) # models imported from gensim, from lemmas, min_count: minimum amount of times it has to co-coccur to make the list of bigrams\n",
    "bigram_mod = models.phrases.Phraser(bigram) # exporting model to a variable so it doesn't use a ton of memory on computer\n",
    "print(bigram_mod[lemmas[0]]) # see if it worked with first song\n",
    "# now mona lisa is considered one word, also broken heart, work of art was turned into bigram (work_art) becase we removed small words like \"of\"\n",
    "# make bigrams for all of the songs\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in text] # for each document in text (for each song in songs), find bigrams and return all together as a single list\n",
    "lemmas = make_bigrams(lemmas)\n",
    "# can also make trigrams, etc\n",
    "# now identify topics\n",
    "# using particular lda approach - topics as probability distributions\n",
    "# find series of probability distributions for words that will describe where words in songs come from\n",
    "# a priori need to know number of topics we're looking for - we don't so usually choose number of topics to make it as interpretable as possible (multiple methods to do this) - topic coherence is commonly used (if they tend to co-occur often)\n",
    "# based on this (look at key for code), we are using five topics\n",
    "# kind of like latent factor analysis\n",
    "# following takes ages to run:\n",
    "ldamodel = models.ldamodel.ldaModel(bow_corpus, num_topics = 5, id2word = dictionary) # bag of words corpus taking in, id numbers is a dictionary\n",
    "# above: unsupervised model\n",
    "ldamodel = models.ldamodel.LdaModel.load('lda5p20_i400.model') # load results (previously run)\n",
    "topics = ldamodel.print_topics(num_words=20) #print_topics command from gensim\n",
    "# print them out\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    \n",
    "# numbers associated with word - weighting\n",
    "# hard to interpret so use function from top (will print out top scoring song for each topic)\n",
    "# need to load bag of words corpus (see key)\n",
    "top_songs_by_topic = top_songs_by_topic(ldamodel, b) # feed in lda model so it knows topic values and bag of words corpus\n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
