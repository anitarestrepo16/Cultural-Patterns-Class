{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is general\n",
    "\n",
    "Spider is more specific - you have to make a unique one for each website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- method is already exisiting function for object\n",
    "- class (in python): define new objects and respective new methods -> gives python its power\n",
    "- class (in HTML): group together things that we want to have same purpose or same way it shows up\n",
    "- HTML: not a language - no logic involved\n",
    "    - any web browser can interpret\n",
    "    - no operations\n",
    "    - just display things\n",
    "    - but what about interactive web pages? CSS - cascading style sheets (helps with design process, visual component)\n",
    "    - JavaScript is the actual language behind websites\n",
    "- HTML + CSS + JavaScript = two components (text/visuals and databases)\n",
    "    - only interested in text for scraping\n",
    "- HTML is a tree\n",
    "    - head is more for CSS - put in how each class should work\n",
    "    - body has tags\n",
    "        - h1 is size\n",
    "        - ps and as: a is anchor\n",
    "- HTML elements\n",
    "    - tags\n",
    "    - attributes\n",
    "    - text (content)\n",
    "- look at HTML file on github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e4/69b87d7827abf03dea2ea984230d50f347b00a7a3897bc93f6ec3dafa494/Scrapy-1.8.0-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 6.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from scrapy) (19.0.0)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
      "Collecting zope.interface>=4.1.3 (from scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/f8/d6f022f17d5c5d229b0003555d2fe7846069ade155d7babefdaa97173281/zope.interface-4.7.1-cp37-cp37m-macosx_10_6_intel.whl (140kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from scrapy) (4.4.1)\n",
      "Collecting service-identity>=16.0.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cryptography>=2.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from scrapy) (2.7)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/4b/c72e7d801facc2f519824680b65d76373e6bb289df668dbf8758ea21ff10/Protego-0.1.15.tar.gz (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Twisted>=17.9.0; python_version >= \"3.5\" (from scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/95/5fff90cd4093c79759d736e5f7c921c8eb7e5057a70d753cdb4e8e5895d7/Twisted-19.10.0.tar.bz2 (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 10.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from scrapy) (1.12.0)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (41.4.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (19.2.0)\n",
      "Collecting pyasn1 (from service-identity>=16.0.0->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 364kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pyasn1-modules (from service-identity>=16.0.0->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: asn1crypto>=0.21.0 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.0.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.12.3)\n",
      "Collecting constantly>=15.1 (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting Automat>=0.3.0 (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/11/756922e977bb296a79ccf38e8d45cafee446733157d59bcd751d3aee57f5/Automat-0.8.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
      "Collecting PyHamcrest>=1.9.0 (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Requirement already satisfied: idna>=2.5 in /Users/Anita/opt/anaconda3/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (2.8)\n",
      "Building wheels for collected packages: PyDispatcher, protego, Twisted\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp37-none-any.whl size=11515 sha256=d78956135c700c5ceac14b4e3b8dc8e37435bc0a66b386eaa03df139e27177c2\n",
      "  Stored in directory: /Users/Anita/Library/Caches/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.15-cp37-none-any.whl size=7734 sha256=f193133e448fc9949ac590d7dcea4f6c8bdb191dda7209fc1970ae7978aeac46\n",
      "  Stored in directory: /Users/Anita/Library/Caches/pip/wheels/72/d1/f2/4e0a2e6d0179c201952b1b3e086a736548605386193cd312f6\n",
      "  Building wheel for Twisted (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Twisted: filename=Twisted-19.10.0-cp37-cp37m-macosx_10_9_x86_64.whl size=3043209 sha256=cbafe5d899c1dd9252dca55d88689d26a89e98d9b71b86bd10e83e5e675ef03c\n",
      "  Stored in directory: /Users/Anita/Library/Caches/pip/wheels/59/31/54/d03a61017b6f2c09942b09c20035007bb64c6f69c9e6be4a6e\n",
      "Successfully built PyDispatcher protego Twisted\n",
      "Installing collected packages: cssselect, w3lib, zope.interface, pyasn1, pyasn1-modules, service-identity, PyDispatcher, parsel, protego, constantly, incremental, Automat, hyperlink, PyHamcrest, Twisted, queuelib, scrapy\n",
      "Successfully installed Automat-0.8.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.10.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 protego-0.1.15 pyasn1-0.4.8 pyasn1-modules-0.2.7 queuelib-1.5.0 scrapy-1.8.0 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use scrapy?\n",
    "- when expect to find differences in pages\n",
    "- spider is a bot\n",
    "\n",
    "How to build spider\n",
    "- for any spider to work, need three ingredients:\n",
    "    - start the request\n",
    "        - need URL\n",
    "    - parse the main page and find the links we need to click to get the text from each one\n",
    "        - need to know what we're going to loop on - put text that we're looking for into a list\n",
    "        - use self because input is actual spider\n",
    "        - to find things we need we can use either HTML or CSS\n",
    "            - xpath: go step by step down the HTML tree (like looking for folders in a path)\n",
    "                - HTML/body/div etc\n",
    "                - using HTML\n",
    "                - can also find things using attributes\n",
    "                - ./ within current directory\n",
    "                - a is anchor\n",
    "                - @ is for attribute\n",
    "    - scrape each site from each link in previous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy # request package already built in\n",
    "class forum_crawl(scrapy.Spider): # making new class forum_crawl that comes from larger class (scrapy.Spider) - called inheritance\n",
    "    name = \"whatever\"\n",
    "    def start_requests(self): #start the request - tell it whih URL and how to parse\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
